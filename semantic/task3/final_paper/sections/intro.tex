\section{Introduction}

Semantic parsing is a process to map natural language utterances onto machine interpretable meaning representations. There has recently been a surge of interest in developing machine learning methods for semantic parsing, due in part to the existence of corpora containing utterances annotated with formal meaning representations. In order to predict the correct logical form for a given utterance, most previous systems rely on predefined templates and manually designed features, which often render the parsing model domain or representation-specific. In this work, we aim to use a simple yet effective method to bridge the gap between natural language and logical form with minimal domain knowledge. And we experiment in an open-domain Dataset, MSParS.\cite{MSParS}

Encoder-decoder architectures based on attention networks have been successfully applied to a variety of NLP tasks ranging from summarization \cite{gehring2017convolutional, narayan2018don} , to machine translation \cite{klein2017opennmt}. In this work, we adopt the general encoder-decoder paradigm to the semantic parsing task. Our model is base on Sequence to Sequence model \cite{sutskever2014sequence} and it learns from natural language descriptions paired with meaning representations; it encodes sentences and decodes logical forms using recurrent neural networks with attention units.
